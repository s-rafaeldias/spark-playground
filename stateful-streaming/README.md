# Stateful processing in Pyspark

## Running the example
```bash
pip install -r requirements.txt
python main.py
```

## Concepts


## References
- https://www.waitingforcode.com/apache-spark-structured-streaming/arbitrary-stateful-processing-pyspark-applyinpandaswithstate/read
- https://spark.apache.org/docs/latest/api//python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandasWithState.html?highlight=applyinpandaswithstate#pyspark.sql.GroupedData.applyInPandasWithState
- https://github.com/bartosz25/spark-playground/tree/master/pyspark-arbitrary-stateful-processing
- https://estuary.dev/stateful-stream-processing/
- https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/concepts/overview/
- https://docs.databricks.com/en/structured-streaming/stateful-streaming.html
- https://www.databricks.com/blog/project-lightspeed-update-advancing-apache-spark-structured-streaming
- https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html
- https://www.databricks.com/blog/2017/10/17/arbitrary-stateful-processing-in-apache-sparks-structured-streaming.html
- https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html
- https://docs.confluent.io/kafka-clients/python/current/overview.html#synchronous-writes
